<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta http-equiv="Cache-Control" content="no-cache, no-store, must-revalidate">
<meta http-equiv="Pragma" content="no-cache">
<meta http-equiv="Expires" content="0">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>SPS DSI Webinar Series: Data sciEnce on GrAphS (DEGAS)</title>
<link rel="stylesheet" type="text/css" media="screen and (max-width: 640px)" href="jemdoc_mobile.css?rnd=1">
<link rel="stylesheet" type="text/css" media="screen and (min-width: 641px)" href="jemdoc.css?rnd=1">
<title></title>
</head>
<body>
<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
try {
    var pageTracker = _gat._getTracker("UA-211439111-1");
    pageTracker._trackPageview();
} catch(err) {}</script>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-item"><a href="#">Go&nbsp;to&nbsp;top</a></div>
<div class="menu-item"><a href="#next">Next&nbsp;Talk</a></div>
<div class="menu-item"><a href="#upcoming">Upcoming&nbsp;Talks</a></div>
<div class="menu-item"><a href="#past">Past&nbsp;Talks</a></div>
</td>
<td id="layout-content">
<h1>SPS DSI Webinar Series: Data sciEnce on GrAphS (DEGAS)</h1>
<table class="imgtablec"><tr><td>
<img src="./images/degas_logo.png" alt="" height="100%" class="center" />&nbsp;</td>
<td align="left"></td></tr></table>
<table class = "dual-mode" cell-padding ="0" cellspacing="">
<tbody>
<tr>
<td class="leftcell">
<div class="blockcontent">This DEGAS Webinar Series is an event initiated by the <a href = "https://signalprocessingsociety.org/community-involvement/data-science-initiative">IEEE SPS Data Science Initiative</a>. The goal is to provide the SP community with updates and advances in learning and inference on graphs. Signal processing and machine learning often deal with data living in regular domains such as space and time. This webinar series will cover the extension of these methods to network data, including topics such as graph filtering, graph sampling, spectral analysis of network data, graph topology identification, geometric deep learning, and so on. Applications can for instance be found in image processing, social networks, epidemics, wireless communications, brain science, recommender systems, and sensor networks.</div>
</td>
<td class="sepcell">&nbsp;</td>
<td class="rightcell">
<a class="twitter-timeline" data-width="500" data-height="225" href="https://twitter.com/DegasSeminar?ref_src=twsrc%5Etfw">Tweets by DegasSeminar</a> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</td>
</tr>
</tbody>
</table>
<a name="next"></a>
<h2>Next Talk</h2>
<table class="imgtable"><tr><td>
<img src="https://petar-v.com/images/headshot.jpg" alt="" width="225px" />&nbsp;</td>
<td align="left"><ul>
<li><p><b>Speaker</b>: <a href="https://petar-v.com" target=&ldquo;blank&rdquo;>Petar Veličković (Deepmind)</a></p>
</li>
<li><p><b>Title</b>: Graph Neural Networks are Dynamic Programmers</p>
</li>
<li><p><b>Date/Time</b>: <b>2:00pm</b>, 14th April, 2022 in Paris Time (<a href="https://time.is/1400_14_April_2022_in_Paris/California/New_York/Hong_Kong" target=&ldquo;blank&rdquo;>@local time</a>, <a href="https://researchseminars.org/seminar/DEGASSeminar/ics" target="“blank”"><img src="./images/ical.png" alt="Add to iCal" height="15px"> Add to your calendar</a>)</p>
</li>
<li><p><b>Abstract</b>: Recent advances in neural algorithmic reasoning with graph neural networks (GNNs) are propped up by the notion of algorithmic alignment. Broadly, a neural network will be better at learning to execute a reasoning task (in terms of sample complexity) if its individual components align well with the target algorithm. Specifically, GNNs are claimed to align with dynamic programming (DP), a general problem-solving strategy which expresses many polynomial-time algorithms. However, has this alignment truly been demonstrated and theoretically quantified? Here we show, using methods from category theory and abstract algebra, that there exists an intricate connection between GNNs and DP, going well beyond the initial observations over individual algorithms such as Bellman-Ford. Exposing this connection, we easily verify several prior findings in the literature, and hope it will serve as a foundation for building stronger algorithmically aligned GNNs.</p>
</li>
</ul>
<ul>
<li><p><b>Speaker Bio</b>: Petar Veličković is a Staff Research Scientist at DeepMind, Affiliated Lecturer at the University of Cambridge, and an Associate of Clare Hall, Cambridge. He holds a PhD in Computer Science from the University of Cambridge (Trinity College), obtained under the supervision of Pietro Liò. Petar's research concerns geometric deep learning—devising neural network architectures that respect the invariances and symmetries in data (a topic he's co-written a proto-book about). For his contributions to the area, Petar is recognised as an ELLIS Scholar in the Geometric Deep Learning Program. Within this area, Petar focusses on graph representation learning and its applications in algorithmic reasoning and computational biology. In particular, he is the first author of Graph Attention Networks—a popular convolutional layer for graphs—and Deep Graph Infomax—a popular self-supervised learning pipeline for graphs (featured in ZDNet). Petar's research has been used in substantially improving the travel-time predictions in Google Maps (featured in the CNBC, Endgadget, VentureBeat, CNET, the Verge and ZDNet), and guiding the intuition of mathematicians towards new top-tier theorems and conjectures (featured in Nature, New Scientist, The Independent, Sky News, The Sunday Times and The Conversation).</p>
</li>
</ul>
<p><ul></ul><ul><a href="https://tudelft.zoom.us/meeting/register/tJwlcuqoqjkoH90a6B-p7bSDgrSpnAuYNxZ3" target="“blank”"><p><img src="./images/register.png" alt="Register Now" width="175"></p></a></ul> </p>
</td></tr></table>
<a name="upcoming"></a>
<h2>Upcoming Talks (all date/time are in Paris Time)</h2>
<p>Click <a href="https://researchseminars.org/seminar/DEGASSeminar" target=&ldquo;blank&rdquo;>here</a> if you would like to import the seminar information to iCal/Google Calendar. </p>
<button type="button" class="collapsible"><b>[+] Speaker: Antonio Ortega (USC)</b> (TBA)</button>
<div class="content">
<table class="imgtable"><tbody><tr><td>
<img src="https://viterbi.usc.edu/directory/images/451bb078ef4d169ecdc20616b903e92b.jpg" alt="" width="150px">&nbsp;</td>
<td align="left"><ul>
<li><p><b>Title</b>: TBA</p>
</li>
<li><p><b>Abstract</b>: TBA</p>
</li>
<li><p><b>Speaker Bio</b>: Antonio Ortega received the Telecommunications Engineering degree from the Universidad Politecnica de Madrid, Madrid, Spain in 1989 and the Ph.D. in Electrical Engineering from Columbia University, New York, NY in 1994. His Ph.D. work was supported by the Fulbright Commission and the Ministry of Education of Spain. He joined the University of Southern California as an Assistant Professor in 1994 and is currently a Professor. He was Director of the Signal and Image Processing Institute and Associate Chair of Electrical Engineering-Systems. In 1995 he received the NSF Faculty Early Career Development (CAREER) Award. He is a Fellow of the IEEE, a member of the ACM and of APSIPA. He has been an Associate Editor of the IEEE Transactions on Image Processing, the IEEE Signal Processing Letters, and Area Editor for Feature Articles, Signal Processing Magazine. He was the technical program co-chair for ICIP 2008 and is technical program co-chair of PCS 2013. He has been a member of the IEEE Signal Processing Society Multimedia Signal Processing (MMSP) and Image and Multidimensional Signal Processing (IMDSP) technical committees. He was Chair of the IMDSP committee in 2004-5. He was the inaugural Editor-in-Chief of the APSIPA Transactions on Signal and Information Processing.

He received the 1997 Northrop Grumman Junior Research Award awarded by the School of Engineering at USC. In 1998 he received the Leonard G. Abraham IEEE Communications Society Prize Paper Award for the best paper published in the IEEE Journal on Selected Areas in Communications in 1997, for his paper co-authored with Chi-Yuan Hsu and Amy R. Reibman. He also received the IEEE Signal Processing Society, Signal Processing Magazine Award in 1999 for a paper co-authored with Kannan Ramchandran, which appeared in the Signal Processing Magazine in November 1998. He also received the 2006 EURASIP Journal on Advances in Signal Processing Best Paper award for his paper A Framework for Adaptive Scalable Video Coding Using Wyner-Ziv Techniques co-authored with Huisheng Wang and Ngai-Man Cheung. His research interests are in the area of digital image and video compression, with a focus on graph signal processing and on systems issues related to transmission over networks, application-specific compression techniques, and fault/error tolerant signal processing algorithms.</p>
</li>
<li><p><a href="https://sites.google.com/usc.edu/stac-lab/home?authuser=1" target="“blank”"><b>Speaker Homepage</b></a></p>
</ul>
</td></tr></tbody></table>
</div>
<a name="past"></a>
<h2>Past Talks</h2>
<p>You can find the recorded seminars on the <a href="https://www.youtube.com/playlist?list=PLcZOnmyqlala5POskgoEkXO7U29yFdYHF" target=&ldquo;blank&rdquo;>IEEE SPS Youtube Channel</a> or the direct links below. You can also find the slides shared by the speakers (if available) by clicking on the expand [+] button.</p>
<button type="button" class="collapsible"><b>[+] Speaker: Baruch Barzel (Bar-Ilan University)</b> (Nov 4, 2021, 3:30pm Paris)</button>
<div class="content">
<table class="imgtable"><tbody><tr><td>
<img src="./images/baruch.jpeg" alt="" width="150px">&nbsp;</td>
<td align="left"><ul>
<li><p><b>Title</b>: Network GPS - A Perturbative Theory of Network Dynamics</p>
</li>
<li><p><b>Abstract</b>: Universal network characteristics, such as the scale-free degree distribution and the small world phenomena, are the bread and butter of network science. But how do we translate such topological findings into an understanding of the system's dynamic behavior: for instance, how does the small world structure impact the patterns of flow in the system? Or how does the presence of hubs affect the distribution of influence? In essence, whether its communicable diseases, genetic regulation or the spread of failures in an infrastructure network, these questions touch on the patterns of information spread in the network. It all begins with a local perturbation, such as a sudden disease outbreak or a local power failure, which then propagates to impact all other nodes. The challenge is that the resulting spatio-temporal propagation patterns are diverse and unpredictable - indeed a Zoo of spreading patterns - that seem to be only loosely connected to the network topology. We show that we can tame this zoo, by exposing a systematic translation of topological elements into their dynamic outcome, allowing us to navigate the network, and, most importantly - to expose a deep universality behind the seemingly diverse dynamics.</p>
</li>
<li><p><b>Speaker Bio</b>: Prof. Baruch Barzel is a physicist and applied mathematician, director of the Complex Network Dynamics lab at Bar-Ilan University. His main research areas are statistical physics, complex systems, nonlinear dynamics and network science. Barzel completed his Ph.D. in physics at the Hebrew University of Jerusalem, Israel as a Hoffman Fellow. He then pursued his postdoctoral training at the Center for Complex Network Research at Northeastern University and at the Channing Division of Network Medicine, Harvard Medical School. Barzel is also an active public lecturer, presenting a weekly corner on Israel National Radio. His research focuses on the dynamic behavior of complex networks, uncovering universal principles that govern the dynamics of diverse systems, such as disease spreading, gene regulatory networks, protein interactions or population dynamics. Prof. Barzel is the recipient of the Racah Prize (2007) and of the Krill Prize on behalf of the Wolf Foundation (2019).</p>
</li>
<li><p><a href="https://www.barzellab.com" target="“blank”"><b>Speaker Homepage</b></a></p></li>
<li><p><a href="https://www.youtube.com/watch?v=QJxsvAY-LIY" target="“blank”"><b>Youtube</b></a>, <a href="./pdf/PresentationBarzel.pdf" target="“blank”"><b>Slides (in PDF)</b></a></p>
</li>
</ul>
</td></tr></tbody></table>
</div>
<button type="button" class="collapsible"><b>[+] Speaker: Antonio G. Marques (King Juan Carlos University)</b> (Nov 18, 2021, 2:00pm Paris)</button>
<div class="content">
<table class="imgtable"><tbody><tr><td>
<img src="./images/marques.jpg" alt="" width="150px">&nbsp;</td>
<td align="left"><ul>
<li><p><b>Title</b>: Connecting the dots: Leveraging GSP to learn graphs from nodal observations</p>
</li>
<li><p><b>Abstract</b>: The talk will provide an overview of graph signal processing (GSP)-based methods designed to learn an unknown network from nodal observations. Using signals to learn a graph is a central problem in network science and statistics, with results going back more than 50 years. The main goal of the talk is threefold: i) explaining in detail fundamental GSP-based methods and comparing those with classical methods in statistics, ii) putting forth a number of GSP-based formulations and algorithms able to address scenarios with a range of different operating conditions, and iii) briefly introducing generalizations to more challenging setups, including multi-layer graphs and learning in the presence of hidden nodal variables. Our graph learning algorithms will be designed as solutions to judiciously formulated constrained-optimization sparse-recovery problems. Critical to this approach is the codification of GSP concepts such as signal smoothness and graph stationarity into tractable constraints. Last but not least, while the focus will be on the so-called network association problem (a setup where observations from all nodes are available), the problem of network tomography (where some nodes remain unobserved, and which can be related to latent-variable graphical lasso) will also be discussed.</p>
</li>
<li><p><b>Speaker Bio</b>: Antonio G. Marques received the telecommunications engineering degree and the Doctorate degree, both with highest honors, from the Carlos III University of Madrid, Spain, in 2002 and 2007, respectively. In 2007, he became a faculty of the Department of Signal Theory and Communications, King Juan Carlos University, Madrid, Spain, where he currently develops his research and teaching activities as a Full Professor and serves as Deputy of the President. From 2005 to 2015, he held different visiting positions at the University of Minnesota, Minneapolis. In 2016 and 2017 he was a visiting scholar at the University of Pennsylvania, Philadelphia. His current research focuses on signal processing for graphs, stochastic network optimization, and machine learning over graphs, with applications to communications, health, and finance. Dr. Marques is a member of IEEE, EURASIP and the ELLIS society, having served these organizations in a number of posts. In recent years he has been an editor of three journals, a member of the IEEE Signal Processing Theory and Methods Technical Committee, a member of the IEEE Signal Processing Big Data Special Interest Group, the Technical Co-Chair of the IEEE CAMSAP Workshop, the General Co-chair of the IEEE Data Science Workshop, and the General Chair of the Graph Signal Processing Workshop. Dr. Marques has also served as an external research proposal evaluator for different organizations, including the Spanish, French, Israeli, Dutch, USA, and Swiss National Science Foundations. His work has received multiple journal and conference paper awards, with recent ones including a 2020 Young Best Paper Award of the IEEE SPS and the "2020 EURASIP Early Career Award", for his "significant contributions to network optimization and graph signal processing".</p>
</li>
<li><p><a href="https://tsc.urjc.es/~amarques/" target="“blank”"><b>Speaker Homepage</b></a></p></li>
<li><p><a href="http://www.youtube.com/watch?v=SAKX95OhvQ4" target="“blank”"><b>Youtube</b></a>, <a href="./pdf/ConnectingTheDots_Talk_2021.pdf" target="“blank”"><b>Slides (in PDF)</b></a></p>
</li>
</ul>
</td></tr></tbody></table>
</div>
<button type="button" class="collapsible"><b>[+] Speaker: Michael Schaub (RWTH Aachen)</b> (Dec 2, 2021, 5:00pm)</button>
<div class="content">
<table class="imgtable"><tbody><tr><td>
<img src="https://michaelschaub.github.io/images/MichaelBW.jpg" alt="" width="150px">&nbsp;</td>
<td align="left"><ul>
<li><p><b>Title</b>: Signal processing on graphs and complexes</p>
</li>
<li><p><b>Abstract</b>: Graph signal processing (GSP) tries to device appropriate tools to process signals supported on graphs by generalizing classical methods from signal processing of time-series and images -- such as smoothing, filtering and supported on the nodes of graphs.  Typically, this involves leveraging the structure of the graph as encoded in the spectral properties of the graph Laplacian. In applications such as traffic network analysis, however, the signals of interest are naturally defined on the edges of a graph, rather than on the nodes. After a very brief recap of the central ideas of GSP, we examine why the standard tools from GSP may not be suitable for the analysis of such edge signals.  More specifically, we discuss how the underlying notion of a 'smooth signal' inherited from (the typically considered variants of) the graph Laplacian are not suitable when dealing with edge signals that encode flows.  To overcome this limitation we devise signal processing tools based on the Hodge-Laplacian and the associated discrete Hodge Theory for simplicial (and cellular) complexes.  We discuss applications of these ideas for signal smoothing, semi-supervised and active learning for edge-flows on discrete (or discretized) spaces.</p>
</li>
<li><p><b>Speaker Bio</b>: Dr. Michael Schaub studied Electrical Engineering and Information Technology at ETH Zurich with a focus on communication systems. After a MSc in Biomedical Engineering at Imperial College London (Neurotechnology stream), he moved to the Mathematics Department of Imperial College London to obtain a PhD in 2014. In the following he worked in Belgium, jointly at the Université catholique de Louvain and at the University of Namur, as a Postdoctoral Research Fellow. In November 2016, Dr. Schaub moved to the Massachusetts Institute of Technology (MIT) as a Postdoctoral Research Associate. From July 2017 onwards he was a Marie Skłodowska Curie Fellow at MIT and the University of Oxford, before joining RWTH Aachen University in June 2020, supported by the NRW Return Programme (2019).</p>
</li>
<li><p><a href="https://michaelschaub.github.io" target="“blank”"><b>Speaker Homepage</b></a></p></li>
<li><p><a href="https://www.youtube.com/watch?v=F_HSgevvvzg" target="“blank”"><b>Youtube</b></a></p></li>
</ul>
</td></tr></tbody></table>
</div>
<button type="button" class="collapsible"><b>[+] Speaker: Jian Tang (MILA/HEC Montreal)</b> (Dec 16, 2021, 4:30pm)</button>
<div class="content">
<table class="imgtable"><tbody><tr><td>
<img src="https://jian-tang.com/images/jian.jpg" alt="" width="150px">&nbsp;</td>
<td align="left"><ul>
<li><p><b>Title</b>: Geometric Deep learning for Drug Discovery</p>
</li>
<li><p><b>Abstract</b>: Drug discovery is a very long and expensive process, taking on average more than 10 years and costing $2.5B to develop a new drug. Artificial intelligence has the potential to significantly accelerate the process of drug discovery by extracting evidence from a huge amount of biomedical data and hence revolutionizes the entire pharmaceutical industry. In particular, graph representation learning and geometric deep learning--a fast growing topic in the machine learning and data mining community focusing on deep learning for graph-structured and 3D data---has seen great opportunities for drug discovery as many data in the domain are represented as graphs or 3D structures (e.g. molecules, proteins, biomedical knowledge graphs). In this talk, I will introduce our recent progress on geometric deep learning for drug discovery and also a newly released open-source machine learning platform for drug discovery, called TorchDrug. </p>
</li>
<li><p><b>Speaker Bio</b>: Jian Tang is currently an assistant professor at Mila-Quebec AI Institute and also at Computer Science Department and Business School of University of Montreal.  He is a Canada CIFAR AI Research Chair. His main research interests are graph representation learning, graph neural networks, geometric deep learning, deep generative models, knowledge graphs and drug discovery. During his PhD, he was awarded with the best paper in ICML2014; in 2016, he was nominated for the best paper award in the top data mining conference World Wide Web (WWW); in 2020, he is awarded with Amazon and Tencent Faculty Research Award. He is one of the most representative researchers in the growing field of graph representation learning and has published a set of representative works in this field such as LINE and RotatE. His work LINE on node representation learning has been widely recognized and is the most cited paper at the WWW conference between 2015 and 2019.  Recently, his group just released an open-source machine learning package, called TorchDrug, aiming at  making AI drug discovery software and libraries freely available to the research community. He is an area chair of ICML and NeurIPS.</p>
</li>
<li><p><a href="https://jian-tang.com" target="“blank”"><b>Speaker Homepage</b></a></p></li>
<li><p><a href="https://www.youtube.com/watch?v=d7CkzgHi_tE" target="“blank”"><b>Youtube</b></a></p></li>
</ul>
</td></tr></tbody></table>
</div>
<button type="button" class="collapsible"><b>[+] Speaker: Peter Battaglia (Deepmind)</b> (Jan 13, 2022, 3:00pm)</button>
<div class="content">
<table class="imgtable"><tbody><tr><td>
<img src="./images/peter.jpeg" alt="" width="150px">&nbsp;</td>
<td align="left"><ul>
<li><p><b>Title</b>: Modeling physical structure and dynamics using graph-based machine learning</p>
</li>
<li><p><b>Abstract</b>: This talk presents various ways complex physical objects and dynamics can be modeled using graph-based machine learning. Graph neural networks and Transformers have grown rapidly in popularity in recent years, and much of the progress has been driven by applications which demand representations and computations which are not well-supported by more traditional deep learning methods. Here we'll explore learning to simulate fluids and non-rigid materials, as well as learning probabilistic generative models of 3D objects and molecules, focusing on how the model choices and innovations were motivated by the structure of the problem and data. The goal of the talk is to emphasize the rich interface between graph-based machine learning and problems that arise in physical settings, and offer practical prescriptions for applying graph-based machine learning to other physical domains.</p>
</li>
<li><p><b>Speaker Bio</b>: Peter Battaglia is a research scientist at DeepMind. Previously he was a postdoc and research scientist in MIT's Department of Brain and Cognitive Sciences. His current work focuses on approaches for reasoning about and interacting with complex systems, by combining richly structured knowledge with flexible learning algorithms.</p>
</li>
<li><p><a href="https://scholar.google.com/citations?user=nQ7Ij30AAAAJ&hl=en" target="“blank”"><b>Speaker Homepage</b></a></p></li>
<li><p><a href="https://www.youtube.com/watch?v=7bVj_BG590M" target="“blank”"><b>Youtube</b></a></p></li>
</ul>
</td></tr></tbody></table>
</div>
<button type="button" class="collapsible"><b>[+] Speaker: Michael Bronstein (University of Oxford / Twitter)</b> (Jan 27, 2022, 2:00pm)</button>
<div class="content">
<table class="imgtable"><tbody><tr><td>
<img src="https://www.imperial.ac.uk/ImageCropToolT4/imageTool/uploaded-images/mbronstein-ae_1598959227010_x1.jpg" alt="" width="150px">&nbsp;</td>
<td align="left"><ul>
<li><p><b>Title</b>: Neural diffusion PDEs, differential geometry, and graph neural networks</p>
</li>
<li><p><b>Abstract</b>: In this talk, I will make connections between Graph Neural Networks (GNNs) and non-Euclidean diffusion equations. I will show that drawing on methods from the domain of differential geometry, it is possible to provide a principled view on such GNN architectural choices as positional encoding and graph rewiring as well as explain and remedy the phenomena of oversquashing and bottlenecks.</p>
</li>
<li><p><b>Speaker Bio</b>: Michael Bronstein is the DeepMind Professor of AI at the University of Oxford and Head of Graph Learning Research at Twitter. He was previously a professor at Imperial College London and held visiting appointments at Stanford, MIT, and Harvard, and has also been affiliated with three Institutes for Advanced Study (at TUM as a Rudolf Diesel Fellow (2017-2019), at Harvard as a Radcliffe fellow (2017-2018), and at Princeton as a short-time scholar (2020)). Michael received his PhD from the Technion in 2007. He is the recipient of the Royal Society Wolfson Research Merit Award, Royal Academy of Engineering Silver Medal, five ERC grants, two Google Faculty Research Awards, and two Amazon AWS ML Research Awards. He is a Member of the Academia Europaea, Fellow of IEEE, IAPR, BCS, and ELLIS, ACM Distinguished Speaker, and World Economic Forum Young Scientist. In addition to his academic career, Michael is a serial entrepreneur and founder of multiple startup companies, including Novafora, Invision (acquired by Intel in 2012), Videocites, and Fabula AI (acquired by Twitter in 2019).</p>
</li>
<li><p><a href="https://www.imperial.ac.uk/people/m.bronstein" target="“blank”"><b>Speaker Homepage</b></a></p></li>
<li><p><a href="https://www.youtube.com/watch?v=77eLJS2c618" target="“blank”"><b>Youtube</b></a></p></li>
</ul>
</td></tr></tbody></table>
</div>
<button type="button" class="collapsible"><b>[+] Speaker: Alexander Jung (Aalto University)</b> (Feb 17, 2022, 3:00pm)</button>
<div class="content">
<table class="imgtable"><tbody><tr><td>
<img src="https://users.aalto.fi/~junga1/AlexHat.jpg" alt="" width="150px">&nbsp;</td>
<td align="left"><ul>
<li><p><b>Title</b>: Federated Learning in Big Data over Networks</p>
</li>
<li><p><b>Abstract</b>: Many important application domains generate distributed collections of heterogeneous local datasets. These local datasets are related via an intrinsic network structure that arises from domain-specific notions of similarity between local datasets. Networked federated learning aims at learning a tailored local model for each local dataset. We formulate networked federated learning using the concept of generalized total variation (GTV) minimization as a regularizer. This formulation unifies and considerably extends recent approaches to federated multi-task learning. We derive precise conditions on the local models as well on their network structure such that our algorithm learns nearly optimal local models. Our analysis reveals an interesting interplay between the (information-) geometry of local models and the (cluster-) geometry of their network.</p>
</li>
<li><p><b>Speaker Bio</b>: Alexander Jung obtained a Ph.D. (with sub auspiciis) in 2012 from Technical University Vienna. After Post-Doc periods at TU Vienna and ETH Zurich, he joined Aalto University as an Assistant Professor for Machine Learning in 2015, He leads the group “Machine Learning for Big Data” which studies explainable machine learning in network structured data.  Alex first-authored a paper that won a Best Student Paper Award at IEEE ICASSP 2011. He received an AWS Machine Learning Research Award and was the "Computer Science Teacher of the Year" at Aalto University in 2018. Currently he serves as an associate editor for the IEEE Signal Processing Letters and as the chair of the IEEE Finland Jt. Chapter on Signal Processing and Circuits and Systems. He authored the textbook "Machine Learning: The Basics" which will be published by Springer in 2022.</p>
</li>
<li><p><a href="https://users.aalto.fi/~junga1/" target="“blank”"><b>Speaker Homepage</b></a></p>
<li><p><a href="https://www.youtube.com/watch?v=JRsuR0h5eLE" target="“blank”"><b>Youtube</b></a>, <a href="./pdf/FMTLBigDataNets_DEGAS.pdf" target="“blank”"><b>Slides (in PDF)</b></a></p>
</li>
</ul>
</td></tr></tbody></table>
</div>
<button type="button" class="collapsible"><b>[+] Speaker: Steven Smith (MIT Lincoln Laboratory)</b> (Mar 3, 2022, 3:00pm)</button>
<div class="content">
<table class="imgtable"><tbody><tr><td>
<img src="https://www.ll.mit.edu/sites/default/files/styles/staff/public/staff/image/2018-11/AR17_Awards_Smith_Steve_504435-1D.png" alt="" width="150px">&nbsp;</td>
<td align="left"><ul>
<li><p><b>Title</b>: Causal Inference on Networks to Characterize Disinformation Narrative Propagation</p>
</li>
<li><p><b>Abstract</b>: The weaponization of digital communications and social media to conduct disinformation campaigns at immense scale, speed, and reach presents new challenges to identify and counter hostile influence operations (IOs).  This talk presents the causal inference approach used as part of an end-to-end framework to automate detection of disinformation narratives, networks, and influential actors. The framework integrates natural language processing, machine learning, graph analytics, and a network causal inference approach to quantify the impact of individual actors in spreading IO narratives.  The impact of each account is inferred by its causal contribution to the overall narrative propagation over the entire network, which is not accurately captured by traditional activity- and topology-based impact statistics.  Impact estimation is based on a method that quantifies each account's unique causal contribution to the overall narrative propagation over the entire network. It accounts for social confounders (e.g., community membership, popularity) and disentangles their effects from the causal estimation using an approach based on the network potential outcome framework.  Because it is impossible to observe the both actual and potential outcomes, the missing potential outcomes must be estimated, which is accomplished using a model.  We demonstrate this approach's capability on real-world hostile IO campaigns with Twitter datasets collected during the 2017 French presidential elections, and known IO accounts disclosed by Twitter over a broad range of IO campaigns (May 2007 to February 2020), over 50,000 accounts, 17 countries, and different account types including both trolls and bots.  Our system classifies IO accounts and networks, and discovers high-impact accounts that escape the lens of traditional impact statistics based on activity counts and network centrality. Results are corroborated with independent sources of known IO accounts from US Congressional reports, investigative journalism, and IO datasets provided by Twitter.</p>
</li>
<li><p><b>Speaker Bio</b>: Dr. Steven Thomas Smith is a senior staff member in the Artificial Intelligence Software Architectures and Algorithms Group at MIT Lincoln Laboratory. He is an expert in radar, sonar, and signal processing who has made pioneering and wide-ranging contributions through his research and technical leadership in estimation theory, resolution limits, and signal processing and optimization on manifolds. He has more than 20 years of experience as an innovative technology leader in statistical data analytics, both theory and practice, and broad leadership experience ranging from first-of-a-kind algorithm development for groundbreaking sensor systems to graph-based intelligence architectures. His contributions span diverse applications from optimum network detection, geometric optimization, geometric acoustics, superresolution, and nonlinear parameter estimation.

Dr. Smith received the SIAM Outstanding Paper Award in 2001 and the IEEE Signal Processing Society Best Paper Award in 2010. He has been an invited speaker as an original inventor of some of the big advances in signal processing over the last decade. He was associate editor of the IEEE Transactions on Signal Processing from 2000 to 2002 and serves on the IEEE Sensor Array and Multichannel and Big Data committees. He has taught signal processing courses at Harvard, MIT, and for the IEEE.

Dr. Smith holds the BASc degree (1986) in electrical engineering and honours mathematics from the University of British Columbia, Vancouver, B.C., and the PhD degree (1993) in applied mathematics from Harvard University, Cambridge, Massachusetts. </p>
</li>
<li><p><a href="https://www.ll.mit.edu/biographies/steven-thomas-smith" target="“blank”"><b>Speaker Homepage</b></a></p>
<li><p><a href="https://youtu.be/ee5b4J3iS3A" target="“blank”"><b>Youtube</b></a></p></li>
</ul>
</td></tr></tbody></table>
</div>
<button type="button" class="collapsible"><b>[+] Speaker: Stefanie Jegelka (MIT)</b> (Mar 17, 2022, 3:30pm)</button>
<div class="content">
<table class="imgtable"><tbody><tr><td>
<img src="https://idss.mit.edu/wp-content/uploads/2015/06/jegelka-re-sized.png" alt="" width="150px">&nbsp;</td>
<td align="left"><ul>
<li><p><b>Title</b>: Learning and Extrapolation in Graph Neural Networks</p>
</li>
<li><p><b>Abstract</b>: Graph Neural Networks (GNNs) have become a popular tool for learning representations of graph-structured inputs, with applications in computational chemistry, recommendation, pharmacy, reasoning, and many other areas. In this talk, I will show some recent results on learning with message-passing GNNs. In particular, GNNs possess important architectural properties and inductive biases that affect learning and generalization. Studying the effect of these inductive biases can be challenging, as they are affected by the architecture (structure and aggregation functions) and training algorithm, together with data and learning task. In particular, we study these biases for learning structured tasks, e.g., simulations or algorithms, and show how for such tasks, architecture choices affect generalization within and outside the training distribution.

This talk is based on joint work with Keyulu Xu, Jingling Li, Mozhi Zhang, Weihua Hu, Vikas Garg, Simon S. Du, Tommi Jaakkola, Jure Leskovec and Ken-ichi Kawarabayashi.</p>
</li>
<li><p><b>Speaker Bio</b>: Stefanie Jegelka is an Associate Professor in the Department of EECS at MIT. Before joining MIT, she was a postdoctoral researcher at UC Berkeley, and obtained her PhD from ETH Zurich and the Max Planck Institute for Intelligent Systems. Stefanie has received a Sloan Research Fellowship, an NSF CAREER Award, a DARPA Young Faculty Award, Google research awards, a Two Sigma faculty research award, the German Pattern Recognition Award and a Best Paper Award at ICML. She has co-organized multiple workshops on (discrete) optimization in machine learning and graph representation learning, and serves as an Action Editor at JMLR and a program chair of ICML 2022. Her research interests span the theory and practice of algorithmic machine learning, in particular, learning problems that involve combinatorial structure.</p>
</li>
<li><p><a href="https://people.csail.mit.edu/stefje/" target="“blank”"><b>Speaker Homepage</b></a></p>
<li><p><a href="https://www.youtube.com/watch?v=1XF5SSXq90w" target="“blank”"><b>Youtube</b></a></p></li>
</ul>
</td></tr></tbody></table>
</div>
<button type="button" class="collapsible"><b>[+] Speaker: Alejandro Ribeiro (UPenn)</b> (Mar 31, 2022, 3:00pm)</button>
<div class="content">
<table class="imgtable"><tbody><tr><td>
<img src="https://alelab.seas.upenn.edu/wp-content/uploads/2017/04/ribeiro-1.jpg" alt="" width="150px">&nbsp;</td>
<td align="left"><ul>
<li><p><b>Title</b>: Learning by Transference in Large Graphs</p>
</li>
<li><p><b>Abstract</b>: Graph neural networks (GNNs) are successful at learning representations for network data but their application to large graphs is constrained by training cost. In this talk we introduce the concept of graph machine learning by transference: Training GNNs on moderate-scale graphs and executing them on large-scale graphs. To study the theoretical underpinnings of learning by transference we consider families of graphs converging to a common graph limit. This so-called graphon is a bounded symmetric kernel which can be interpreted as both a random graph model and a limit object of a convergent sequence of graphs. Graphs sampled from a graphon almost surely share structural properties in the limit, which implies that graphons describe families of similar graphs. We can thus expect that processing data supported on graphs associated with the same graphon should yield similar results. In this talk we formalize this intuition by showing that the error made when transferring a GNN across two graphs in a graphon family is small when the graphs are sufficiently large.</p>
</li>
<li><p><b>Speaker Bio</b>: Alejandro Ribeiro received the B.Sc. degree in electrical engineering from the Universidad de la República Oriental del Uruguay in 1998 and the M.Sc. and Ph.D. degrees in electrical engineering from the Department of Electrical and Computer Engineering at the University of Minnesota in 2005 and 2007. He joined the University of Pennsylvania (Penn) in 2008 where he is currently Professor of Electrical and Systems Engineering. His research is in wireless autonomous networks, machine learning on network data and distributed collaborative learning. Papers coauthored by Dr. Ribeiro received the 2014 O. Hugo Schuck best paper award, and paper awards at EUSIPCO 2021, ICASSP 2020, EUSIPCO 2019, CDC 2017, SSP Workshop 2016, SAM Workshop 2016, Asilomar SSC Conference 2015, ACC 2013, ICASSP 2006, and ICASSP 2005. His teaching has been recognized with the 2017 Lindback award for distinguished teaching and the 2012 S. Reid Warren, Jr. Award presented by Penn’s undergraduate student body for outstanding teaching. Dr. Ribeiro is a Fulbright scholar class of 2003 and Penn Fellow class of 2015.</p>
</li>
<li><p><a href="https://alelab.seas.upenn.edu" target="“blank”"><b>Speaker Homepage</b></a></p></li>
<li><p><a href="https://www.youtube.com/watch?v=OLBymKF61N0" target="“blank”"><b>Youtube</b></a>, <a href="./pdf/Alejandro_Slides.pdf" target="“blank”"><b>Slides</b></a></p></li>
</ul>
</td></tr></tbody></table>
</div>
<h2>Organization</h2>
<p>The DEGAS Webinar Series is organized by <a href="https://cas.tudelft.nl/People/bio.php?id=3" target=&ldquo;blank&rdquo;>Geert Leus</a>, <a href="https://people.epfl.ch/dorina.thanou?lang=en" target=&ldquo;blank&rdquo;>Dorina Thanou</a>, <a href="http://www1.se.cuhk.edu.hk/~htwai/" target=&ldquo;blank&rdquo;>Hoi-To Wai</a> as a part of the <a href="https://signalprocessingsociety.org/community-involvement/data-science-initiative" target=&ldquo;blank&rdquo;>IEEE SPS Data Science Initiative</a>. <a href="https://twitter.com/DegasSeminar?ref_src=twsrc%5Etfw" class="twitter-follow-button" data-show-count="false">Follow @DegasSeminar</a><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></p>
<table class="imgtable"><tr><td>
<img src="./images/sps.png" alt="" height="75%" />&nbsp;</td>
<td align="left"></td></tr></table>
<script>
var coll = document.getElementsByClassName("collapsible");
var i;

for (i = 0; i < coll.length; i++) {
  coll[i].addEventListener("click", function() {
    this.classList.toggle("active");
    var content = this.nextElementSibling;
    if (content.style.display === "block") {
      content.style.display = "none";
    } else {
      content.style.display = "block";
    }
  });
}
</script>
<div id="footer">
<div id="footer-text">
Page generated 2022-04-08 10:42:37 HKT, by <a href="https://github.com/wsshin/jemdoc_mathjax" target="blank">jemdoc+MathJax</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
