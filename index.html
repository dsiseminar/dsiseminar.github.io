<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta http-equiv="Cache-Control" content="no-cache, no-store, must-revalidate">
<meta http-equiv="Pragma" content="no-cache">
<meta http-equiv="Expires" content="0">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>SPS DSI Webinar Series: Data sciEnce on GrAphS (DEGAS)</title>
<link rel="stylesheet" type="text/css" media="screen and (max-width: 640px)" href="jemdoc_mobile.css?rnd=1">
<link rel="stylesheet" type="text/css" media="screen and (min-width: 641px)" href="jemdoc.css?rnd=1">
<title></title>
</head>
<body>
<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
try {
    var pageTracker = _gat._getTracker("UA-211439111-1");
    pageTracker._trackPageview();
} catch(err) {}</script>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-item"><a href="#">Go&nbsp;to&nbsp;top</a></div>
<div class="menu-item"><a href="#next">Next&nbsp;Talk</a></div>
<div class="menu-item"><a href="#upcoming">Upcoming&nbsp;Talks</a></div>
<div class="menu-item"><a href="#past">Past&nbsp;Talks</a></div>
</td>
<td id="layout-content">
<h1>SPS DSI Webinar Series: Data sciEnce on GrAphS (DEGAS)</h1>
<table class="imgtablec"><tr><td>
<img src="./images/degas_logo.png" alt="" height="100%" class="center" />&nbsp;</td>
<td align="left"></td></tr></table>
<table class = "dual-mode" cell-padding ="0" cellspacing="">
<tbody>
<tr>
<td class="leftcell">
<div class="blockcontent">This DEGAS Webinar Series is an event initiated by the <a href = "https://signalprocessingsociety.org/community-involvement/data-science-initiative">IEEE SPS Data Science Initiative</a>. The goal is to provide the SP community with updates and advances in learning and inference on graphs. Signal processing and machine learning often deal with data living in regular domains such as space and time. This webinar series will cover the extension of these methods to network data, including topics such as graph filtering, graph sampling, spectral analysis of network data, graph topology identification, geometric deep learning, and so on. Applications can for instance be found in image processing, social networks, epidemics, wireless communications, brain science, recommender systems, and sensor networks.</div>
</td>
<td class="sepcell">&nbsp;</td>
<td class="rightcell">
<a class="twitter-timeline" data-width="500" data-height="225" href="https://twitter.com/DegasSeminar?ref_src=twsrc%5Etfw">Tweets by DegasSeminar</a> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</td>
</tr>
</tbody>
</table>
<a name="next"></a>
<h2>Next Talk</h2>
<table class="imgtable"><tr><td>
<img src="./images/selin.jpg" alt="" width="225px" />&nbsp;</td>
<td align="left"><ul>
<li><p><b>Speaker</b>: <a href="https://sites.google.com/msu.edu/aviyente/home" target=&ldquo;blank&rdquo;>Selin Aviyente (Michigan State University)</a></p>
</li>
<li><p><b>Title</b>: Single view and Multiview Signed Graph Learning: Applications to gene regulatory network inference</p>
</li>
<li><p><b>Date/Time</b>: 3:00pm, 30th November, 2022 in Paris Time (<a href="https://time.is/1500_30_November_2022_in_Paris/California/New_York/Hong_Kong" target=&ldquo;blank&rdquo;>@local time</a>, <a href="https://researchseminars.org/seminar/DEGASSeminar/ics" target="“blank”"><img src="./images/ical.png" alt="Add to iCal" height="15px"> Add to your calendar</a>)</p>
</li>
<li><p><b>Abstract</b>: In many modern data science applications, relationships between data samples are well described with a graph structure. While many real-world data are intrinsically graph-structured, there is a large number of applications, such as single cell gene expression data, where the graph is not readily available and needs to be learned from a set of observations, i.e. graph signals. Most of the existing work on graph learning considers homogeneous data, where all signals are assumed to be defined by a single graph structure, and undirected graphs. In this talk, I will introduce a framework for signed graph learning (SGL) using the following two assumptions. Signal values on nodes connected by positive edges are similar to each other, i.e., variation over positive edges is small, while signal values on nodes connected by negative edges are dissimilar to each other, i.e., variation over negative edges is large. From graph signal processing perspective, these assumptions correspond to graph signals being low- and high-frequency over positive and negative edges, respectively. Next, this framework will be extended to heterogeneous data that come from multiple related graphs introducing multiview signed graph learning (mvSGL). mvSGL learns multiple signed graphs jointly while regularizing the learned graphs to be similar to each other, where the similarity between different views is ensured through a consensus graph, which captures the common structure across views. The proposed framework is evaluated on both simulated real single cell RNA sequencing data.</p>
</li>
<li><p><b>Speaker Bio</b>: Selin Aviyente received her B.S. degree in Electrical and Electronics engineering from Bogazici University, Istanbul in 1997; M.S. and Ph.D. degrees, both in Electrical Engineering: Systems, from the University of Michigan, Ann Arbor, in 1999 and 2002, respectively. She joined the Department of Electrical and Computer Engineering at Michigan State University in 2002, where she is currently a Professor and Associate Chair for Undergraduate Studies. Her research focuses on statistical and nonstationary signal processing, higher-order data representations and network science with applications to biological signals. She has authored more than 150 peer-reviewed journal and conference papers. She is the recipient of a 2005 Withrow Teaching Excellence Award, a 2008 NSF CAREER Award and 2021 Withrow Excellence in Diversity Award. She is currently serving as the chair of IEEE Signal Processing Society Bioimaging and Signal Processing Technical Committee, on the Steering Committees of IEEE SPS Data Science Initiative and IEEE BRAIN. She has served as an Associate Editor and Senior Area Editor for IEEE Transactions on Signal Processing, IEEE Transactions on Signal and Information Processing over Networks, IEEE Open Journal of Signal Processing and Digital Signal Processing.</p>
</li>
</ul>
<p><ul></ul><ul><a href="https://tudelft.zoom.us/meeting/register/tJwlcuqoqjkoH90a6B-p7bSDgrSpnAuYNxZ3" target="“blank”"><p><img src="./images/register.png" alt="Register Now" width="175"></p></a></ul> </p>
</td></tr></table>
<a name="upcoming"></a>
<h2>Upcoming Talks (all date/time are in Paris Time)</h2>
<p>Click <a href="https://researchseminars.org/seminar/DEGASSeminar" target=&ldquo;blank&rdquo;>here</a> if you would like to import the seminar information to iCal/Google Calendar. </p>
<button type="button" class="collapsible"><b>[+] Speaker: Haggai Maron (NVIDIA Research)</b> (Dec 14, 2022, 3:00pm Paris)</button>
<div class="content">
<table class="imgtable"><tbody><tr><td>
<img src="https://haggaim.github.io/images/haggai.png" alt="" width="150px">&nbsp;</td>
<td align="left"><ul>
<li><p><b>Title</b>: Subgraph-based networks for expressive, efficient, and domain-independent graph learning</p>
</li>
<li><p><b>Abstract</b>: While message-passing neural networks (MPNNs) are the most popular architectures for graph learning, their expressive power is inherently limited. In order to gain increased expressive power while retaining efficiency, several recent works apply MPNNs to subgraphs of the original graph. As a starting point, the talk will introduce the Equivariant Subgraph Aggregation Networks (ESAN) architecture, which is a representative framework for this class of methods. In ESAN, each graph is represented as a set of subgraphs, selected according to a predefined policy. The sets of subgraphs are then processed using an equivariant architecture designed specifically for this purpose. I will then present a recent follow-up work that revisits the symmetry group suggested in ESAN and suggests that a more precise choice can be made if we restrict our attention to a specific popular family of subgraph selection policies. We will see that using this observation, one can make a direct connection between subgraph GNNs and Invariant Graph Networks (IGNs), thus providing new insights into subgraph GNNs' expressive power and design space. The talk is based on our ICLR and NeurIPS 2022 papers (spotlight and oral presentations accordingly).</p>
</li>
<li><p><b>Speaker Bio</b>: Haggai is a Senior Research Scientist at NVIDIA Research and a member of NVIDIA's TLV lab. His main field of interest is machine learning in structured domains. In particular, he works on applying deep learning to sets, graphs, point clouds, and surfaces, usually by leveraging their symmetry structure. He completed his Ph.D. in 2019 at the Weizmann Institute of Science under the supervision of Prof. Yaron Lipman. Haggai will be joining the Faculty of Electrical and Computer Engineering at the Technion as an Assistant Professor in 2023.</p>
</li>
<li><p><a href="https://haggaim.github.io" target="“blank”"><b>Speaker's Homepage</b></a></p></li>
</ul>
</td></tr></tbody></table>
</div>
<button type="button" class="collapsible"><b>[+] Speaker: Stephan Gunnemann (Technical University of Munich)</b> (TBA)</button>
<div class="content">
<table class="imgtable"><tbody><tr><td>
<img src="https://www.ias.tum.de/fileadmin/_processed_/b/c/csm_Guennemann_Stephan_IAS_86dff3cc96.png" alt="" width="150px">&nbsp;</td>
<td align="left"><ul>
<li><p><b>Title</b>: TBA</p>
</li>
<li><p><b>Abstract</b>: TBA.</p>
</li>
<li><p><b>Speaker Bio</b>: TBA.</p>
</li>
<li><p><a href="https://www.ias.tum.de/en/ias/guennemann-stephan/" target="“blank”"><b>Speaker's Homepage</b></a></p></li>
</ul>
</td></tr></tbody></table>
</div>
<button type="button" class="collapsible"><b>[+] Speaker: Dimitri Van de Ville (EPFL)</b> (TBA)</button>
<div class="content">
<table class="imgtable"><tbody><tr><td>
<img src="https://miplab.epfl.ch/application/files/thumbnails/xlarge/4015/1725/9299/PHOTO_VanDeVille6_3.jpg" alt="" width="150px">&nbsp;</td>
<td align="left"><ul>
<li><p><b>Title</b>: TBA</p>
</li>
<li><p><b>Abstract</b>: TBA.</p>
</li>
<li><p><b>Speaker Bio</b>: TBA.</p>
</li>
<li><p><a href="https://miplab.epfl.ch/index.php/people/vandeville" target="“blank”"><b>Speaker's Homepage</b></a></p></li>
</ul>
</td></tr></tbody></table>
</div>
<a name="past"></a>
<h2>Past Talks</h2>
<p>You can find the recorded seminars on the <a href="https://www.youtube.com/playlist?list=PLcZOnmyqlala5POskgoEkXO7U29yFdYHF" target=&ldquo;blank&rdquo;>IEEE SPS Youtube Channel</a> or the direct links below. You can also find the slides shared by the speakers (if available) by clicking on the expand [+] button.</p>
<button type="button" class="collapsible"><b>[+] Speaker: Baruch Barzel (Bar-Ilan University)</b> (Nov 4, 2021, 3:30pm Paris)</button>
<div class="content">
<table class="imgtable"><tbody><tr><td>
<img src="./images/baruch.jpeg" alt="" width="150px">&nbsp;</td>
<td align="left"><ul>
<li><p><b>Title</b>: Network GPS - A Perturbative Theory of Network Dynamics</p>
</li>
<li><p><b>Abstract</b>: Universal network characteristics, such as the scale-free degree distribution and the small world phenomena, are the bread and butter of network science. But how do we translate such topological findings into an understanding of the system's dynamic behavior: for instance, how does the small world structure impact the patterns of flow in the system? Or how does the presence of hubs affect the distribution of influence? In essence, whether its communicable diseases, genetic regulation or the spread of failures in an infrastructure network, these questions touch on the patterns of information spread in the network. It all begins with a local perturbation, such as a sudden disease outbreak or a local power failure, which then propagates to impact all other nodes. The challenge is that the resulting spatio-temporal propagation patterns are diverse and unpredictable - indeed a Zoo of spreading patterns - that seem to be only loosely connected to the network topology. We show that we can tame this zoo, by exposing a systematic translation of topological elements into their dynamic outcome, allowing us to navigate the network, and, most importantly - to expose a deep universality behind the seemingly diverse dynamics.</p>
</li>
<li><p><b>Speaker Bio</b>: Prof. Baruch Barzel is a physicist and applied mathematician, director of the Complex Network Dynamics lab at Bar-Ilan University. His main research areas are statistical physics, complex systems, nonlinear dynamics and network science. Barzel completed his Ph.D. in physics at the Hebrew University of Jerusalem, Israel as a Hoffman Fellow. He then pursued his postdoctoral training at the Center for Complex Network Research at Northeastern University and at the Channing Division of Network Medicine, Harvard Medical School. Barzel is also an active public lecturer, presenting a weekly corner on Israel National Radio. His research focuses on the dynamic behavior of complex networks, uncovering universal principles that govern the dynamics of diverse systems, such as disease spreading, gene regulatory networks, protein interactions or population dynamics. Prof. Barzel is the recipient of the Racah Prize (2007) and of the Krill Prize on behalf of the Wolf Foundation (2019).</p>
</li>
<li><p><a href="https://www.barzellab.com" target="“blank”"><b>Speaker Homepage</b></a></p></li>
<li><p><a href="https://www.youtube.com/watch?v=QJxsvAY-LIY" target="“blank”"><b>Youtube</b></a>, <a href="./pdf/PresentationBarzel.pdf" target="“blank”"><b>Slides (in PDF)</b></a></p>
</li>
</ul>
</td></tr></tbody></table>
</div>
<button type="button" class="collapsible"><b>[+] Speaker: Antonio G. Marques (King Juan Carlos University)</b> (Nov 18, 2021, 2:00pm Paris)</button>
<div class="content">
<table class="imgtable"><tbody><tr><td>
<img src="./images/marques.jpg" alt="" width="150px">&nbsp;</td>
<td align="left"><ul>
<li><p><b>Title</b>: Connecting the dots: Leveraging GSP to learn graphs from nodal observations</p>
</li>
<li><p><b>Abstract</b>: The talk will provide an overview of graph signal processing (GSP)-based methods designed to learn an unknown network from nodal observations. Using signals to learn a graph is a central problem in network science and statistics, with results going back more than 50 years. The main goal of the talk is threefold: i) explaining in detail fundamental GSP-based methods and comparing those with classical methods in statistics, ii) putting forth a number of GSP-based formulations and algorithms able to address scenarios with a range of different operating conditions, and iii) briefly introducing generalizations to more challenging setups, including multi-layer graphs and learning in the presence of hidden nodal variables. Our graph learning algorithms will be designed as solutions to judiciously formulated constrained-optimization sparse-recovery problems. Critical to this approach is the codification of GSP concepts such as signal smoothness and graph stationarity into tractable constraints. Last but not least, while the focus will be on the so-called network association problem (a setup where observations from all nodes are available), the problem of network tomography (where some nodes remain unobserved, and which can be related to latent-variable graphical lasso) will also be discussed.</p>
</li>
<li><p><b>Speaker Bio</b>: Antonio G. Marques received the telecommunications engineering degree and the Doctorate degree, both with highest honors, from the Carlos III University of Madrid, Spain, in 2002 and 2007, respectively. In 2007, he became a faculty of the Department of Signal Theory and Communications, King Juan Carlos University, Madrid, Spain, where he currently develops his research and teaching activities as a Full Professor and serves as Deputy of the President. From 2005 to 2015, he held different visiting positions at the University of Minnesota, Minneapolis. In 2016 and 2017 he was a visiting scholar at the University of Pennsylvania, Philadelphia. His current research focuses on signal processing for graphs, stochastic network optimization, and machine learning over graphs, with applications to communications, health, and finance. Dr. Marques is a member of IEEE, EURASIP and the ELLIS society, having served these organizations in a number of posts. In recent years he has been an editor of three journals, a member of the IEEE Signal Processing Theory and Methods Technical Committee, a member of the IEEE Signal Processing Big Data Special Interest Group, the Technical Co-Chair of the IEEE CAMSAP Workshop, the General Co-chair of the IEEE Data Science Workshop, and the General Chair of the Graph Signal Processing Workshop. Dr. Marques has also served as an external research proposal evaluator for different organizations, including the Spanish, French, Israeli, Dutch, USA, and Swiss National Science Foundations. His work has received multiple journal and conference paper awards, with recent ones including a 2020 Young Best Paper Award of the IEEE SPS and the "2020 EURASIP Early Career Award", for his "significant contributions to network optimization and graph signal processing".</p>
</li>
<li><p><a href="https://tsc.urjc.es/~amarques/" target="“blank”"><b>Speaker Homepage</b></a></p></li>
<li><p><a href="http://www.youtube.com/watch?v=SAKX95OhvQ4" target="“blank”"><b>Youtube</b></a>, <a href="./pdf/ConnectingTheDots_Talk_2021.pdf" target="“blank”"><b>Slides (in PDF)</b></a></p>
</li>
</ul>
</td></tr></tbody></table>
</div>
<button type="button" class="collapsible"><b>[+] Speaker: Michael Schaub (RWTH Aachen)</b> (Dec 2, 2021, 5:00pm)</button>
<div class="content">
<table class="imgtable"><tbody><tr><td>
<img src="https://michaelschaub.github.io/images/MichaelBW.jpg" alt="" width="150px">&nbsp;</td>
<td align="left"><ul>
<li><p><b>Title</b>: Signal processing on graphs and complexes</p>
</li>
<li><p><b>Abstract</b>: Graph signal processing (GSP) tries to device appropriate tools to process signals supported on graphs by generalizing classical methods from signal processing of time-series and images -- such as smoothing, filtering and supported on the nodes of graphs.  Typically, this involves leveraging the structure of the graph as encoded in the spectral properties of the graph Laplacian. In applications such as traffic network analysis, however, the signals of interest are naturally defined on the edges of a graph, rather than on the nodes. After a very brief recap of the central ideas of GSP, we examine why the standard tools from GSP may not be suitable for the analysis of such edge signals.  More specifically, we discuss how the underlying notion of a 'smooth signal' inherited from (the typically considered variants of) the graph Laplacian are not suitable when dealing with edge signals that encode flows.  To overcome this limitation we devise signal processing tools based on the Hodge-Laplacian and the associated discrete Hodge Theory for simplicial (and cellular) complexes.  We discuss applications of these ideas for signal smoothing, semi-supervised and active learning for edge-flows on discrete (or discretized) spaces.</p>
</li>
<li><p><b>Speaker Bio</b>: Dr. Michael Schaub studied Electrical Engineering and Information Technology at ETH Zurich with a focus on communication systems. After a MSc in Biomedical Engineering at Imperial College London (Neurotechnology stream), he moved to the Mathematics Department of Imperial College London to obtain a PhD in 2014. In the following he worked in Belgium, jointly at the Université catholique de Louvain and at the University of Namur, as a Postdoctoral Research Fellow. In November 2016, Dr. Schaub moved to the Massachusetts Institute of Technology (MIT) as a Postdoctoral Research Associate. From July 2017 onwards he was a Marie Skłodowska Curie Fellow at MIT and the University of Oxford, before joining RWTH Aachen University in June 2020, supported by the NRW Return Programme (2019).</p>
</li>
<li><p><a href="https://michaelschaub.github.io" target="“blank”"><b>Speaker Homepage</b></a></p></li>
<li><p><a href="https://www.youtube.com/watch?v=F_HSgevvvzg" target="“blank”"><b>Youtube</b></a></p></li>
</ul>
</td></tr></tbody></table>
</div>
<button type="button" class="collapsible"><b>[+] Speaker: Jian Tang (MILA/HEC Montreal)</b> (Dec 16, 2021, 4:30pm)</button>
<div class="content">
<table class="imgtable"><tbody><tr><td>
<img src="https://jian-tang.com/images/jian.jpg" alt="" width="150px">&nbsp;</td>
<td align="left"><ul>
<li><p><b>Title</b>: Geometric Deep learning for Drug Discovery</p>
</li>
<li><p><b>Abstract</b>: Drug discovery is a very long and expensive process, taking on average more than 10 years and costing $2.5B to develop a new drug. Artificial intelligence has the potential to significantly accelerate the process of drug discovery by extracting evidence from a huge amount of biomedical data and hence revolutionizes the entire pharmaceutical industry. In particular, graph representation learning and geometric deep learning--a fast growing topic in the machine learning and data mining community focusing on deep learning for graph-structured and 3D data---has seen great opportunities for drug discovery as many data in the domain are represented as graphs or 3D structures (e.g. molecules, proteins, biomedical knowledge graphs). In this talk, I will introduce our recent progress on geometric deep learning for drug discovery and also a newly released open-source machine learning platform for drug discovery, called TorchDrug. </p>
</li>
<li><p><b>Speaker Bio</b>: Jian Tang is currently an assistant professor at Mila-Quebec AI Institute and also at Computer Science Department and Business School of University of Montreal.  He is a Canada CIFAR AI Research Chair. His main research interests are graph representation learning, graph neural networks, geometric deep learning, deep generative models, knowledge graphs and drug discovery. During his PhD, he was awarded with the best paper in ICML2014; in 2016, he was nominated for the best paper award in the top data mining conference World Wide Web (WWW); in 2020, he is awarded with Amazon and Tencent Faculty Research Award. He is one of the most representative researchers in the growing field of graph representation learning and has published a set of representative works in this field such as LINE and RotatE. His work LINE on node representation learning has been widely recognized and is the most cited paper at the WWW conference between 2015 and 2019.  Recently, his group just released an open-source machine learning package, called TorchDrug, aiming at  making AI drug discovery software and libraries freely available to the research community. He is an area chair of ICML and NeurIPS.</p>
</li>
<li><p><a href="https://jian-tang.com" target="“blank”"><b>Speaker Homepage</b></a></p></li>
<li><p><a href="https://www.youtube.com/watch?v=d7CkzgHi_tE" target="“blank”"><b>Youtube</b></a></p></li>
</ul>
</td></tr></tbody></table>
</div>
<button type="button" class="collapsible"><b>[+] Speaker: Peter Battaglia (Deepmind)</b> (Jan 13, 2022, 3:00pm)</button>
<div class="content">
<table class="imgtable"><tbody><tr><td>
<img src="./images/peter.jpeg" alt="" width="150px">&nbsp;</td>
<td align="left"><ul>
<li><p><b>Title</b>: Modeling physical structure and dynamics using graph-based machine learning</p>
</li>
<li><p><b>Abstract</b>: This talk presents various ways complex physical objects and dynamics can be modeled using graph-based machine learning. Graph neural networks and Transformers have grown rapidly in popularity in recent years, and much of the progress has been driven by applications which demand representations and computations which are not well-supported by more traditional deep learning methods. Here we'll explore learning to simulate fluids and non-rigid materials, as well as learning probabilistic generative models of 3D objects and molecules, focusing on how the model choices and innovations were motivated by the structure of the problem and data. The goal of the talk is to emphasize the rich interface between graph-based machine learning and problems that arise in physical settings, and offer practical prescriptions for applying graph-based machine learning to other physical domains.</p>
</li>
<li><p><b>Speaker Bio</b>: Peter Battaglia is a research scientist at DeepMind. Previously he was a postdoc and research scientist in MIT's Department of Brain and Cognitive Sciences. His current work focuses on approaches for reasoning about and interacting with complex systems, by combining richly structured knowledge with flexible learning algorithms.</p>
</li>
<li><p><a href="https://scholar.google.com/citations?user=nQ7Ij30AAAAJ&hl=en" target="“blank”"><b>Speaker Homepage</b></a></p></li>
<li><p><a href="https://www.youtube.com/watch?v=7bVj_BG590M" target="“blank”"><b>Youtube</b></a></p></li>
</ul>
</td></tr></tbody></table>
</div>
<button type="button" class="collapsible"><b>[+] Speaker: Michael Bronstein (University of Oxford / Twitter)</b> (Jan 27, 2022, 2:00pm)</button>
<div class="content">
<table class="imgtable"><tbody><tr><td>
<img src="https://www.imperial.ac.uk/ImageCropToolT4/imageTool/uploaded-images/mbronstein-ae_1598959227010_x1.jpg" alt="" width="150px">&nbsp;</td>
<td align="left"><ul>
<li><p><b>Title</b>: Neural diffusion PDEs, differential geometry, and graph neural networks</p>
</li>
<li><p><b>Abstract</b>: In this talk, I will make connections between Graph Neural Networks (GNNs) and non-Euclidean diffusion equations. I will show that drawing on methods from the domain of differential geometry, it is possible to provide a principled view on such GNN architectural choices as positional encoding and graph rewiring as well as explain and remedy the phenomena of oversquashing and bottlenecks.</p>
</li>
<li><p><b>Speaker Bio</b>: Michael Bronstein is the DeepMind Professor of AI at the University of Oxford and Head of Graph Learning Research at Twitter. He was previously a professor at Imperial College London and held visiting appointments at Stanford, MIT, and Harvard, and has also been affiliated with three Institutes for Advanced Study (at TUM as a Rudolf Diesel Fellow (2017-2019), at Harvard as a Radcliffe fellow (2017-2018), and at Princeton as a short-time scholar (2020)). Michael received his PhD from the Technion in 2007. He is the recipient of the Royal Society Wolfson Research Merit Award, Royal Academy of Engineering Silver Medal, five ERC grants, two Google Faculty Research Awards, and two Amazon AWS ML Research Awards. He is a Member of the Academia Europaea, Fellow of IEEE, IAPR, BCS, and ELLIS, ACM Distinguished Speaker, and World Economic Forum Young Scientist. In addition to his academic career, Michael is a serial entrepreneur and founder of multiple startup companies, including Novafora, Invision (acquired by Intel in 2012), Videocites, and Fabula AI (acquired by Twitter in 2019).</p>
</li>
<li><p><a href="https://www.imperial.ac.uk/people/m.bronstein" target="“blank”"><b>Speaker Homepage</b></a></p></li>
<li><p><a href="https://www.youtube.com/watch?v=77eLJS2c618" target="“blank”"><b>Youtube</b></a></p></li>
</ul>
</td></tr></tbody></table>
</div>
<button type="button" class="collapsible"><b>[+] Speaker: Alexander Jung (Aalto University)</b> (Feb 17, 2022, 3:00pm)</button>
<div class="content">
<table class="imgtable"><tbody><tr><td>
<img src="https://users.aalto.fi/~junga1/AlexHat.jpg" alt="" width="150px">&nbsp;</td>
<td align="left"><ul>
<li><p><b>Title</b>: Federated Learning in Big Data over Networks</p>
</li>
<li><p><b>Abstract</b>: Many important application domains generate distributed collections of heterogeneous local datasets. These local datasets are related via an intrinsic network structure that arises from domain-specific notions of similarity between local datasets. Networked federated learning aims at learning a tailored local model for each local dataset. We formulate networked federated learning using the concept of generalized total variation (GTV) minimization as a regularizer. This formulation unifies and considerably extends recent approaches to federated multi-task learning. We derive precise conditions on the local models as well on their network structure such that our algorithm learns nearly optimal local models. Our analysis reveals an interesting interplay between the (information-) geometry of local models and the (cluster-) geometry of their network.</p>
</li>
<li><p><b>Speaker Bio</b>: Alexander Jung obtained a Ph.D. (with sub auspiciis) in 2012 from Technical University Vienna. After Post-Doc periods at TU Vienna and ETH Zurich, he joined Aalto University as an Assistant Professor for Machine Learning in 2015, He leads the group “Machine Learning for Big Data” which studies explainable machine learning in network structured data.  Alex first-authored a paper that won a Best Student Paper Award at IEEE ICASSP 2011. He received an AWS Machine Learning Research Award and was the "Computer Science Teacher of the Year" at Aalto University in 2018. Currently he serves as an associate editor for the IEEE Signal Processing Letters and as the chair of the IEEE Finland Jt. Chapter on Signal Processing and Circuits and Systems. He authored the textbook "Machine Learning: The Basics" which will be published by Springer in 2022.</p>
</li>
<li><p><a href="https://users.aalto.fi/~junga1/" target="“blank”"><b>Speaker Homepage</b></a></p>
<li><p><a href="https://www.youtube.com/watch?v=JRsuR0h5eLE" target="“blank”"><b>Youtube</b></a>, <a href="./pdf/FMTLBigDataNets_DEGAS.pdf" target="“blank”"><b>Slides (in PDF)</b></a></p>
</li>
</ul>
</td></tr></tbody></table>
</div>
<button type="button" class="collapsible"><b>[+] Speaker: Steven Smith (MIT Lincoln Laboratory)</b> (Mar 3, 2022, 3:00pm)</button>
<div class="content">
<table class="imgtable"><tbody><tr><td>
<img src="https://www.ll.mit.edu/sites/default/files/styles/staff/public/staff/image/2018-11/AR17_Awards_Smith_Steve_504435-1D.png" alt="" width="150px">&nbsp;</td>
<td align="left"><ul>
<li><p><b>Title</b>: Causal Inference on Networks to Characterize Disinformation Narrative Propagation</p>
</li>
<li><p><b>Abstract</b>: The weaponization of digital communications and social media to conduct disinformation campaigns at immense scale, speed, and reach presents new challenges to identify and counter hostile influence operations (IOs).  This talk presents the causal inference approach used as part of an end-to-end framework to automate detection of disinformation narratives, networks, and influential actors. The framework integrates natural language processing, machine learning, graph analytics, and a network causal inference approach to quantify the impact of individual actors in spreading IO narratives.  The impact of each account is inferred by its causal contribution to the overall narrative propagation over the entire network, which is not accurately captured by traditional activity- and topology-based impact statistics.  Impact estimation is based on a method that quantifies each account's unique causal contribution to the overall narrative propagation over the entire network. It accounts for social confounders (e.g., community membership, popularity) and disentangles their effects from the causal estimation using an approach based on the network potential outcome framework.  Because it is impossible to observe the both actual and potential outcomes, the missing potential outcomes must be estimated, which is accomplished using a model.  We demonstrate this approach's capability on real-world hostile IO campaigns with Twitter datasets collected during the 2017 French presidential elections, and known IO accounts disclosed by Twitter over a broad range of IO campaigns (May 2007 to February 2020), over 50,000 accounts, 17 countries, and different account types including both trolls and bots.  Our system classifies IO accounts and networks, and discovers high-impact accounts that escape the lens of traditional impact statistics based on activity counts and network centrality. Results are corroborated with independent sources of known IO accounts from US Congressional reports, investigative journalism, and IO datasets provided by Twitter.</p>
</li>
<li><p><b>Speaker Bio</b>: Dr. Steven Thomas Smith is a senior staff member in the Artificial Intelligence Software Architectures and Algorithms Group at MIT Lincoln Laboratory. He is an expert in radar, sonar, and signal processing who has made pioneering and wide-ranging contributions through his research and technical leadership in estimation theory, resolution limits, and signal processing and optimization on manifolds. He has more than 20 years of experience as an innovative technology leader in statistical data analytics, both theory and practice, and broad leadership experience ranging from first-of-a-kind algorithm development for groundbreaking sensor systems to graph-based intelligence architectures. His contributions span diverse applications from optimum network detection, geometric optimization, geometric acoustics, superresolution, and nonlinear parameter estimation.

Dr. Smith received the SIAM Outstanding Paper Award in 2001 and the IEEE Signal Processing Society Best Paper Award in 2010. He has been an invited speaker as an original inventor of some of the big advances in signal processing over the last decade. He was associate editor of the IEEE Transactions on Signal Processing from 2000 to 2002 and serves on the IEEE Sensor Array and Multichannel and Big Data committees. He has taught signal processing courses at Harvard, MIT, and for the IEEE.

Dr. Smith holds the BASc degree (1986) in electrical engineering and honours mathematics from the University of British Columbia, Vancouver, B.C., and the PhD degree (1993) in applied mathematics from Harvard University, Cambridge, Massachusetts. </p>
</li>
<li><p><a href="https://www.ll.mit.edu/biographies/steven-thomas-smith" target="“blank”"><b>Speaker Homepage</b></a></p>
<li><p><a href="https://youtu.be/ee5b4J3iS3A" target="“blank”"><b>Youtube</b></a></p></li>
</ul>
</td></tr></tbody></table>
</div>
<button type="button" class="collapsible"><b>[+] Speaker: Stefanie Jegelka (MIT)</b> (Mar 17, 2022, 3:30pm)</button>
<div class="content">
<table class="imgtable"><tbody><tr><td>
<img src="https://idss.mit.edu/wp-content/uploads/2015/06/jegelka-re-sized.png" alt="" width="150px">&nbsp;</td>
<td align="left"><ul>
<li><p><b>Title</b>: Learning and Extrapolation in Graph Neural Networks</p>
</li>
<li><p><b>Abstract</b>: Graph Neural Networks (GNNs) have become a popular tool for learning representations of graph-structured inputs, with applications in computational chemistry, recommendation, pharmacy, reasoning, and many other areas. In this talk, I will show some recent results on learning with message-passing GNNs. In particular, GNNs possess important architectural properties and inductive biases that affect learning and generalization. Studying the effect of these inductive biases can be challenging, as they are affected by the architecture (structure and aggregation functions) and training algorithm, together with data and learning task. In particular, we study these biases for learning structured tasks, e.g., simulations or algorithms, and show how for such tasks, architecture choices affect generalization within and outside the training distribution.

This talk is based on joint work with Keyulu Xu, Jingling Li, Mozhi Zhang, Weihua Hu, Vikas Garg, Simon S. Du, Tommi Jaakkola, Jure Leskovec and Ken-ichi Kawarabayashi.</p>
</li>
<li><p><b>Speaker Bio</b>: Stefanie Jegelka is an Associate Professor in the Department of EECS at MIT. Before joining MIT, she was a postdoctoral researcher at UC Berkeley, and obtained her PhD from ETH Zurich and the Max Planck Institute for Intelligent Systems. Stefanie has received a Sloan Research Fellowship, an NSF CAREER Award, a DARPA Young Faculty Award, Google research awards, a Two Sigma faculty research award, the German Pattern Recognition Award and a Best Paper Award at ICML. She has co-organized multiple workshops on (discrete) optimization in machine learning and graph representation learning, and serves as an Action Editor at JMLR and a program chair of ICML 2022. Her research interests span the theory and practice of algorithmic machine learning, in particular, learning problems that involve combinatorial structure.</p>
</li>
<li><p><a href="https://people.csail.mit.edu/stefje/" target="“blank”"><b>Speaker Homepage</b></a></p>
<li><p><a href="https://www.youtube.com/watch?v=1XF5SSXq90w" target="“blank”"><b>Youtube</b></a></p></li>
</ul>
</td></tr></tbody></table>
</div>
<button type="button" class="collapsible"><b>[+] Speaker: Alejandro Ribeiro (UPenn)</b> (Mar 31, 2022, 3:00pm)</button>
<div class="content">
<table class="imgtable"><tbody><tr><td>
<img src="https://alelab.seas.upenn.edu/wp-content/uploads/2017/04/ribeiro-1.jpg" alt="" width="150px">&nbsp;</td>
<td align="left"><ul>
<li><p><b>Title</b>: Learning by Transference in Large Graphs</p>
</li>
<li><p><b>Abstract</b>: Graph neural networks (GNNs) are successful at learning representations for network data but their application to large graphs is constrained by training cost. In this talk we introduce the concept of graph machine learning by transference: Training GNNs on moderate-scale graphs and executing them on large-scale graphs. To study the theoretical underpinnings of learning by transference we consider families of graphs converging to a common graph limit. This so-called graphon is a bounded symmetric kernel which can be interpreted as both a random graph model and a limit object of a convergent sequence of graphs. Graphs sampled from a graphon almost surely share structural properties in the limit, which implies that graphons describe families of similar graphs. We can thus expect that processing data supported on graphs associated with the same graphon should yield similar results. In this talk we formalize this intuition by showing that the error made when transferring a GNN across two graphs in a graphon family is small when the graphs are sufficiently large.</p>
</li>
<li><p><b>Speaker Bio</b>: Alejandro Ribeiro received the B.Sc. degree in electrical engineering from the Universidad de la República Oriental del Uruguay in 1998 and the M.Sc. and Ph.D. degrees in electrical engineering from the Department of Electrical and Computer Engineering at the University of Minnesota in 2005 and 2007. He joined the University of Pennsylvania (Penn) in 2008 where he is currently Professor of Electrical and Systems Engineering. His research is in wireless autonomous networks, machine learning on network data and distributed collaborative learning. Papers coauthored by Dr. Ribeiro received the 2014 O. Hugo Schuck best paper award, and paper awards at EUSIPCO 2021, ICASSP 2020, EUSIPCO 2019, CDC 2017, SSP Workshop 2016, SAM Workshop 2016, Asilomar SSC Conference 2015, ACC 2013, ICASSP 2006, and ICASSP 2005. His teaching has been recognized with the 2017 Lindback award for distinguished teaching and the 2012 S. Reid Warren, Jr. Award presented by Penn’s undergraduate student body for outstanding teaching. Dr. Ribeiro is a Fulbright scholar class of 2003 and Penn Fellow class of 2015.</p>
</li>
<li><p><a href="https://alelab.seas.upenn.edu" target="“blank”"><b>Speaker Homepage</b></a></p></li>
<li><p><a href="https://www.youtube.com/watch?v=OLBymKF61N0" target="“blank”"><b>Youtube</b></a>, <a href="./pdf/Alejandro_Slides.pdf" target="“blank”"><b>Slides</b></a></p></li>
</ul>
</td></tr></tbody></table>
</div>
<button type="button" class="collapsible"><b>[+] Speaker: Petar Veličković (Deepmind)</b> (Apr 14, 2022, 2:00pm)</button>
<div class="content">
<table class="imgtable"><tbody><tr><td>
<img src="https://petar-v.com/images/headshot.jpg" alt="" width="150px">&nbsp;</td>
<td align="left"><ul>
<li><p><b>Title</b>: Graph Neural Networks are Dynamic Programmers</p>
</li>
<li><p><b>Abstract</b>: Recent advances in neural algorithmic reasoning with graph neural networks (GNNs) are propped up by the notion of algorithmic alignment. Broadly, a neural network will be better at learning to execute a reasoning task (in terms of sample complexity) if its individual components align well with the target algorithm. Specifically, GNNs are claimed to align with dynamic programming (DP), a general problem-solving strategy which expresses many polynomial-time algorithms. However, has this alignment truly been demonstrated and theoretically quantified? Here we show, using methods from category theory and abstract algebra, that there exists an intricate connection between GNNs and DP, going well beyond the initial observations over individual algorithms such as Bellman-Ford. Exposing this connection, we easily verify several prior findings in the literature, and hope it will serve as a foundation for building stronger algorithmically aligned GNNs.</p>
</li>
<li><p><b>Speaker Bio</b>: Petar Veličković is a Staff Research Scientist at DeepMind, Affiliated Lecturer at the University of Cambridge, and an Associate of Clare Hall, Cambridge. He holds a PhD in Computer Science from the University of Cambridge (Trinity College), obtained under the supervision of Pietro Liò. Petar's research concerns geometric deep learning—devising neural network architectures that respect the invariances and symmetries in data (a topic he's co-written a proto-book about). For his contributions to the area, Petar is recognised as an ELLIS Scholar in the Geometric Deep Learning Program. Within this area, Petar focusses on graph representation learning and its applications in algorithmic reasoning and computational biology. In particular, he is the first author of Graph Attention Networks—a popular convolutional layer for graphs—and Deep Graph Infomax—a popular self-supervised learning pipeline for graphs (featured in ZDNet). Petar's research has been used in substantially improving the travel-time predictions in Google Maps (featured in the CNBC, Endgadget, VentureBeat, CNET, the Verge and ZDNet), and guiding the intuition of mathematicians towards new top-tier theorems and conjectures (featured in Nature, New Scientist, The Independent, Sky News, The Sunday Times and The Conversation).</p>
</li>
<li><p><a href="https://petar-v.com" target="“blank”"><b>Speaker Homepage</b></a></p></li>
<li><p><a href="https://www.youtube.com/watch?v=_CkTl2uoaS4" target="“blank”"><b>Youtube</b></a></li>
</ul>
</td></tr></tbody></table>
</div>
<button type="button" class="collapsible"><b>[+] Speaker: Antonio Ortega (USC)</b> (May 5, 2022, 6:00pm Paris)</button>
<div class="content">
<table class="imgtable"><tbody><tr><td>
<img src="https://viterbi.usc.edu/directory/images/451bb078ef4d169ecdc20616b903e92b.jpg" alt="" width="150px">&nbsp;</td>
<td align="left"><ul>
<li><p><b>Title</b>: Graph Constructions for Machine Learning Applications: New Insights and Algorithms</p>
</li>
<li><p><b>Abstract</b>: Graphs have long been used in a wide variety of problems, such as in analysis of social networks, machine learning, network protocol optimization or image processing. In the last few years, a growing body of work has been developed to extend and complement well known concepts in spectral graph theory, leading to
the emergence of Graph Signal Processing (GSP) as a broad research field. In this talk we focus on summarizing recent results that lead to a GSP perspective of machine learning problems. The key observation is that representations of sample data points (e.g., images in a training set) can be used to construct graphs, with nodes representing samples, label information resulting in graph signals, and edge weights capturing the relative positions of samples in feature space. We will first review how this perspective has been used in well known techniques for label propagation and semi-supervised learning. Then, we will introduce the non-negative kernel regression (NNK) graph construction, describe its properties, and introduce example applications in machine learning areas such as i) model explainability, ii) local interpolative classification and iii) self-supervised learning.</p>
</li>
<li><p><b>Speaker Bio</b>: Antonio Ortega received the Telecommunications Engineering degree from the Universidad Politecnica de Madrid, Madrid, Spain in 1989 and the Ph.D. in Electrical Engineering from Columbia University, New York, NY in 1994. His Ph.D. work was supported by the Fulbright Commission and the Ministry of Education of Spain. He joined the University of Southern California as an Assistant Professor in 1994 and is currently a Professor. He was Director of the Signal and Image Processing Institute and Associate Chair of Electrical Engineering-Systems. In 1995 he received the NSF Faculty Early Career Development (CAREER) Award. He is a Fellow of the IEEE, a member of the ACM and of APSIPA. He has been an Associate Editor of the IEEE Transactions on Image Processing, the IEEE Signal Processing Letters, and Area Editor for Feature Articles, Signal Processing Magazine. He was the technical program co-chair for ICIP 2008 and is technical program co-chair of PCS 2013. He has been a member of the IEEE Signal Processing Society Multimedia Signal Processing (MMSP) and Image and Multidimensional Signal Processing (IMDSP) technical committees. He was Chair of the IMDSP committee in 2004-5. He was the inaugural Editor-in-Chief of the APSIPA Transactions on Signal and Information Processing.

He received the 1997 Northrop Grumman Junior Research Award awarded by the School of Engineering at USC. In 1998 he received the Leonard G. Abraham IEEE Communications Society Prize Paper Award for the best paper published in the IEEE Journal on Selected Areas in Communications in 1997, for his paper co-authored with Chi-Yuan Hsu and Amy R. Reibman. He also received the IEEE Signal Processing Society, Signal Processing Magazine Award in 1999 for a paper co-authored with Kannan Ramchandran, which appeared in the Signal Processing Magazine in November 1998. He also received the 2006 EURASIP Journal on Advances in Signal Processing Best Paper award for his paper A Framework for Adaptive Scalable Video Coding Using Wyner-Ziv Techniques co-authored with Huisheng Wang and Ngai-Man Cheung. His research interests are in the area of digital image and video compression, with a focus on graph signal processing and on systems issues related to transmission over networks, application-specific compression techniques, and fault/error tolerant signal processing algorithms.</p>
</li>
<li><p><a href="https://sites.google.com/usc.edu/stac-lab/home?authuser=1" target="“blank”"><b>Speaker Homepage</b></a></p></li>
<li><p><a href="https://www.youtube.com/watch?v=goWmmXfjhBY" target="“blank”"><b>Youtube</b></a></li>
</ul>
</td></tr></tbody></table>
</div>
<button type="button" class="collapsible"><b>[+] Speaker: Usman A. Khan (Tufts University)</b> (May 19, 2022, 4:00pm Paris)</button>
<div class="content">
<table class="imgtable"><tbody><tr><td>
<img src="https://www.eecs.tufts.edu/~khan/images/khan_pic.JPG" alt="" width="150px">&nbsp;</td>
<td align="left"><ul>
<li><p><b>Title</b>: Distributed stochastic non-convex optimization: Optimal regimes and tradeoffs</p>
</li>
<li><p><b>Abstract</b>: In many emerging applications, it is of paramount interest to learn hidden parameters from data. For example, self-driving cars may use onboard cameras to identify pedestrians, highway lanes, or traffic signs in various light and weather conditions. Problems such as these can be framed as classification, regression, or risk minimization in general, at the heart of which lies stochastic optimization and machine learning. In many practical scenarios, distributed and decentralized learning methods are preferable as they benefit from a divide-and-conquer approach towards storage and computation at the expense of local (short-range) communication. In this talk, I will present our recent work that develops a novel algorithmic framework to address various aspects of distributed stochastic first-order optimization methods for non-convex problems. A major focus will be to characterize regimes where distributed solutions outperform their centralized counterparts and lead to optimal convergence guarantees. Moreover, I will characterize certain desirable attributes of distributed methods in the context of linear speedup and network-independent convergence rates. Throughout the talk, I will demonstrate such key aspects of the proposed methods with the help of provable theoretical results and numerical experiments on real data.</p>
</li>
<li><p><b>Speaker Bio</b>: Usman A. Khan is a Professor (effective Sep. 2022) of Electrical and Computer Engineering (ECE) at Tufts University, USA. His research interests include signal processing, stochastic systems, optimization and control, and machine learning. He has published extensively in these topics with more than 120 articles in journals and conference proceedings and holds multiple patents. Recognition of his work includes the prestigious National Science Foundation (NSF) Career award, several federally funded projects and NSF REU awards, an IEEE journal cover, and several news articles including two in IEEE spectrum. He received the 2021 EURASIP Best Paper Award for a paper in the EURASIP Journal on Advances
in Signal Processing, and Best Student Paper awards at the 2014 IEEE International Conference on Networking, Sensing and Control, and at the IEEE Asilomar Conference in 2014 and 2016.
Dr. Khan received his B.S. degree in 2002 from University of Engineering and Technology, Pakistan, M.S. degree in 2004 from University of Wisconsin-Madison, USA, and Ph.D. degree in 2009 from Carnegie Mellon University, USA, all in ECE. He was a postdoc in the GRASP lab at the University of Pennsylvania and also has held a Visiting Professor position at KTH, Sweden. Dr. Khan is an IEEE Senior Member and is an elected full member of the Sensor Array and Multichannel Technical Committee with the IEEE Signal Processing Society since 2019, where he was an Associate member from 2010 to 2019. He was an elected full member of the IEEE Big Data Special Interest Group from 2017 to 2019, and has served on the IEEE Young Professionals Committee and on the IEEE Technical Activities Board. He has served as an Associate Editor on several publications: IEEE Transactions on Smart Grid (2014-2017); IEEE Control System Letters (2018-2020), IEEE Transactions on Signal and Information Processing over Networks (2019-current); IEEE Open Journal of Signal Processing (2019-current); and IEEE Transactions on Signal Processing (2021-current). He served as the Chief Editor for the Proceedings of the IEEE special issue on Optimization for Data-driven Learning and Control (Nov. 2020), and as a Guest Associate Editor for the IEEE Control System Letters special issue on Learning and Control (Nov. 2020). He served as the Technical Area Chair for the Networks track in 2020 IEEE Asilomar Conference on Signals Systems and Computers and for the Signal Processing for Self Aware and Social Autonomous Systems track at the 1st IEEE International Conference on Autonomous Systems (Aug. 2021).</p>
</li>
<li><p><a href="https://www.eecs.tufts.edu/~khan/" target="“blank”"><b>Speaker Homepage</b></a></p></li>
<li><p><a href="https://www.youtube.com/watch?v=8q-cKk3SnJA" target="“blank”"><b>Youtube</b></a></li>
</ul>
</td></tr></tbody></table>
</div>
<button type="button" class="collapsible"><b>[+] Speaker: Santiago Segarra (Rice University)</b> (June 9, 4:00pm Paris)</button>
<div class="content">
<table class="imgtable"><tbody><tr><td>
<img src="https://cpb-us-e1.wpmucdn.com/blogs.rice.edu/dist/a/9284/files/2019/06/photo_webpage.jpg" alt="" width="150px">&nbsp;</td>
<td align="left"><ul>
<li><p><b>Title</b>: Principled Simplicial Neural Networks for Trajectory Prediction</p>
</li>
<li><p><b>Abstract</b>: We consider the construction of neural network architectures for data on simplicial complexes. In studying maps on the chain complex of a simplicial complex, we define three desirable properties of a simplicial neural network architecture: namely, permutation equivariance, orientation equivariance, and simplicial awareness. The first two properties respectively account for the fact that the node indexing and the simplex orientations are arbitrary. The last property encodes the desirable feature that the output of the neural network depends on the entire simplicial complex and not on a subset of its dimensions. Based on these properties, we propose a simple convolutional architecture, rooted in tools from algebraic topology, for the problem of trajectory prediction, and show that it obeys all three of these properties when an odd, nonlinear activation function is used. We then demonstrate the effectiveness of this architecture in extrapolating trajectories on synthetic and real datasets, with particular emphasis on the gains in generalizability to unseen trajectories.</p>
</li>
<li><p><b>Speaker Bio</b>: Santiago Segarra received the B.Sc. degree in Industrial Engineering with highest honors (Valedictorian) from the Instituto Tecnológico de Buenos Aires (ITBA), Argentina, in 2011, the M.Sc. in Electrical Engineering from the University of Pennsylvania (Penn), Philadelphia, in 2014 and the Ph.D. degree in Electrical and Systems Engineering from Penn in 2016. From September 2016 to June 2018 he was a postdoctoral research associate with the Institute for Data, Systems, and Society at the Massachusetts Institute of Technology. Since July 2018, Dr. Segarra is a W. M. Rice Trustee Assistant Professor in the Department of Electrical and Computer Engineering at Rice University. His research interests include network theory, data analysis, machine learning, and graph signal processing. Dr. Segarra received the ITBA’s 2011 Best Undergraduate Thesis Award in Industrial Engineering, the 2011 Outstanding Graduate Award granted by the National Academy of Engineering of Argentina, the 2017 Penn’s Joseph and Rosaline Wolf Award for Best Doctoral Dissertation in Electrical and Systems Engineering, the 2020 IEEE Signal Processing Society Young Author Best Paper Award, the 2021 Rice’s School of Engineering Research + Teaching Excellence Award, and five best conference paper awards.</p>
</li>
<li><p><a href="https://segarra.rice.edu" target="“blank”"><b>Speaker Homepage</b></a></p></li>
<li><p><a href="https://www.youtube.com/watch?v=bjgfi0Y5xFM" target="“blank”"><b>Youtube</b></a></li>
</ul>
</td></tr></tbody></table>
</div>
<button type="button" class="collapsible"><b>[+] Speaker: Panagiotis A. Traganitis & Georgios B. Giannakis (MSU and UMN)</b> (Oct 19, 2022, 3:00pm Paris)</button>
<div class="content">
<table class="imgtable"><tbody><tr>
<td>
<img src="./images/panosyogos.png" alt="" width="150px">&nbsp;</td>
<td align="left"><ul>
<li><p><b>Title</b>: Learning from Unreliable Labels via Crowdsourcing</p>
</li>
<li><p><b>Abstract</b>: Crowdsourcing has emerged as a powerful paradigm for tackling various machine learning, data mining, and data science tasks, by enlisting inexpensive crowds of human workers, or annotators, to accomplish learning and inference tasks. While conceptually related to distributed data and decision fusion, crowdsourcing seeks to not only aggregate information from multiple human annotators or unreliable (a.k.a. weak) sources, but to also assess their reliabilities. Crowdsourcing can thus be readily adapted to information fusion tasks in contested environments, where data may be provided from unreliable and even adversarial agents. Focusing on the classification task, exposition will include label aggregation, moments of annotator responses, dependencies of dynamic networked data, Gaussian Process-, and Deep Learning-based crowdsourcing demonstrated through extensive tests.</p>
</li>
<li><p><b>Speaker Bio</b>: Panagiotis A. Traganitis received his Diploma in Electrical and Computer Engineering from the National Technical University of Athens, Greece in 2013, his M.Sc. in Electrical Engineering in 2015 and his Ph.D. in Electrical Engineering in 2019, both from the University of Minnesota (UMN), Twin Cities. From 2019 to 2022, he was a postdoctoral researcher with the Department of Electrical and Computer Engineering (ECE) at the University of Minnesota, Twin Cities. In August 2022, he joined the ECE department at Michigan State University as an Assistant Professor. His research interests include statistical signal processing and learning, crowdsourcing and weak supervision, distributed learning, big data analytics, and network science.

Georgios B. Giannakis received his Diploma in Electrical Engr. (EE) from the Ntl. Tech. U. of Athens, Greece, 1981. From 1982 to 1986 he was with the U. of Southern California (USC), where he received his MSc. in EE, 1983, MSc. in Mathematics, 1986, and Ph.D. in EE, 1986. He was with the U. of Virginia from 1987 to 1998, and since 1999 he has been with the U. of Minnesota (UMN), where he held an Endowed Chair of Telecommunications, served as director of the Digital Technology Center from 2008 to 2021, and since 2016 he has been a UMN Presidential Chair in ECE.

His interests span the areas of statistical learning, communications, and networking - subjects on which he has published more than 485 journal papers, 790 conference papers, 25 book chapters, two edited books and two research monographs. Current research focuses on Data Science with applications to IoT, and power networks with renewables. He is the (co-) inventor of 36 issued patents, and the (co- )recipient of 10 best journal paper awards from the IEEE Signal Processing (SP) and Communications Societies, including the G. Marconi Prize. He also received the IEEE-SPS ‘Nobert Wiener’ Society Award (2019); EURASIP’s ‘A. Papoulis’ Society Award (2020); Tech. Achievement Awards from the IEEE-SPS (2000), and from EURASIP (2005); the IEEE ComSoc Education Award (2019); and the IEEE Fourier Technical Field Award (2015). He is a member of the Academia Europaea, the Academy of Athens, Greece, and Fellow of the US Ntl. Academy of Inventors, the European Academy of Sciences, IEEE, and EURASIP. He has served the IEEE in a number of posts, including that of a Distinguished Lecturer for the IEEE-SPS.</p>
</li>
<li><p><a href="https://www.traganitis.com" target="“blank”"><b>Panagiotis's Homepage</b></a>, <a href="http://spincom.umn.edu/" target="“blank”"><b>Georgios's Homepage</b></a></p></li>
<li><p><a href="https://www.youtube.com/watch?v=C3euFvz5L54" target="“blank”"><b>Youtube</b></a></li>
</ul>
</td></tr></tbody></table>
</div>
<button type="button" class="collapsible"><b>[+] Speaker: Sergio Barbarossa (Sapienza University of Rome)</b> (Nov 2, 2022, 3:00pm Paris)</button>
<div class="content">
<table class="imgtable"><tbody><tr><td>
<img src="https://sites.google.com/a/uniroma1.it/sergiobarbarossa/_/rsrc/1529509885058/home/IMG_0226R.jpg" alt="" width="150px">&nbsp;</td>
<td align="left"><ul>
<li><p><b>Title</b>: Topological signal processing and learning</p>
</li>
<li><p><b>Abstract</b>: In the last years, processing signals defined over graphs has been a research topic that has attracted a lot of interest because of its widespread applications. Graphs are a simple example of a topological space, incorporating only pairwise relations. The goal of this lecture is to generalize graph signal processing to handle signals defined over more general topological spaces incorporating multiway relations of any order. In the first part of this lecture, I will start introducing simplicial and cell complexes as examples of spaces possessing a rich algebraic structure that facilitates the extraction of useful information from signals defined on them. I will motivate the introduction of a simplicial (or cell) Fourier Transform and show how to design filters and derive effective sampling strategies over a cell complex. Then I will present methods to infer the structure of the space from data. In the second part of the lecture, I will move to topological learning, describing the design of topological neural networks, operating on data living on a topological space. A number of applications will be presented to highlight the potentials of the proposed methods.</p>
</li>
<li><p><b>Speaker Bio</b>: Sergio Barbarossa is a Full Professor at Sapienza University of Rome and a Senior Research Fellow of Sapienza School for Advanced Studies (SSAS). He is an IEEE Fellow and a EURASIP Fellow. He received the IEEE Best Paper Awards from the IEEE Signal Processing Society in the years 2000, 2014, and 2020, and the Technical Achievements Award from the European Association for Signal Processing (EURASIP) society in 2010. He served as an IEEE Distinguished Lecturer and has been the scientific coordinator of several european projects. His main current research interests include topological signal processing and learning, semantic and goal-oriented communications, 6G networks and distributed edge machine learning.</p>
</li>
<li><p><a href="https://sites.google.com/a/uniroma1.it/sergiobarbarossa/" target="“blank”"><b>Speaker's Homepage</b></a></p></li>
<li><p><a href="https://www.youtube.com/watch?v=Ylwz6BWeIvA" target="“blank”"><b>Youtube</b></a></li>
</ul>
</td></tr></tbody></table>
</div>
<button type="button" class="collapsible"><b>[+] Speaker: Gonzalo Mateos (University of Rochester)</b> (Nov 16, 2022, 3:00pm Paris)</button>
<div class="content">
<table class="imgtable"><tbody><tr><td>
<img src="https://www.hajim.rochester.edu/ece/sites/gmateos//postre.jpg" alt="" width="150px">&nbsp;</td>
<td align="left"><ul>
<li><p><b>Title</b>: Graph adjacency spectral embeddings: Algorithmic advances and applications</p>
</li>
<li><p><b>Abstract</b>: The random dot product graph (RDPG) is a tractable yet expressive generative graph model for relational data, that subsumes simple Erdős-Rényi and stochastic block model ensembles as particular cases. RDPGs postulate that there exist latent positions for each node and specify the edge formation probabilities via the inner product of the corresponding latent vectors. In this talk, we first focus on the embedding task of estimating these latent positions from observed graphs. The workhorse adjacency spectral embedding (ASE) offers an approximate solution obtained via the eigendecomposition of the adjacency matrix, which enjoys solid statistical guarantees but can be computationally intensive and is formally solving a surrogate problem. To address these challenges, we bring to bear recent non-convex optimization advances and demonstrate their impact to RDPG inference. We show the proposed algorithms are scalable, robust to missing network data, and can track the latent positions over time when the graphs are acquired in a streaming fashion; even for dynamic networks subject to node additions and deletions. We also discuss extensions to the vanilla RDPG to accommodate directed and weighted graphs. Unlike previous proposals, our non-parametric RDPG model for weighted networks does not require a priori specification of the weights’ distribution to perform inference and estimation in a provably consistent fashion. Finally, we discuss the problem of online monitoring and detection of changes in the underlying data distribution of a graph sequence. Our idea is to endow sequential change-point detection (CPD) techniques with a graph representation learning substrate based on the versatile RDPG model. We share an open-source implementation of the proposed node embedding and online CPD algorithms, whose effectiveness is demonstrated via synthetic and real network data experiments.</p>
</li>
<li><p><b>Speaker Bio</b>: Gonzalo Mateos earned the B.Sc. degree from Universidad de la Republica, Uruguay, in 2005, and the M.Sc. and Ph.D. degrees from the University of Minnesota, Twin Cities, in 2009 and 2011, all in electrical engineering. He joined the University of Rochester, Rochester, NY, in 2014, where he is currently an Associate Professor with the Department of Electrical and Computer Engineering, as well as an Asaro Biggar Family Fellow in Data Science. During the 2013 academic year, he was a visiting scholar with the Computer Science Department at Carnegie Mellon University. From 2004 to 2006, he worked as a Systems Engineer at Asea Brown Boveri (ABB), Uruguay. His research interests lie in the areas of statistical learning from complex data, network science, decentralized optimization, and graph signal processing, with applications in brain connectivity, dynamic network health monitoring, social, power grid, and Big Data analytics.</p>
</li>
<li><p><a href="https://www.hajim.rochester.edu/ece/sites/gmateos//" target="“blank”"><b>Speaker's Homepage</b></a></p></li>
</ul>
</td></tr></tbody></table>
</div>
<h2>Organization</h2>
<p>The DEGAS Webinar Series is organized by <a href="https://cas.tudelft.nl/People/bio.php?id=3" target=&ldquo;blank&rdquo;>Geert Leus</a>, <a href="https://people.epfl.ch/dorina.thanou?lang=en" target=&ldquo;blank&rdquo;>Dorina Thanou</a>, <a href="http://www1.se.cuhk.edu.hk/~htwai/" target=&ldquo;blank&rdquo;>Hoi-To Wai</a> as a part of the <a href="https://signalprocessingsociety.org/community-involvement/data-science-initiative" target=&ldquo;blank&rdquo;>IEEE SPS Data Science Initiative</a>. <a href="https://twitter.com/DegasSeminar?ref_src=twsrc%5Etfw" class="twitter-follow-button" data-show-count="false">Follow @DegasSeminar</a><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></p>
<table class="imgtable"><tr><td>
<img src="./images/sps.png" alt="" height="75%" />&nbsp;</td>
<td align="left"></td></tr></table>
<script>
var coll = document.getElementsByClassName("collapsible");
var i;

for (i = 0; i < coll.length; i++) {
  coll[i].addEventListener("click", function() {
    this.classList.toggle("active");
    var content = this.nextElementSibling;
    if (content.style.display === "block") {
      content.style.display = "none";
    } else {
      content.style.display = "block";
    }
  });
}
</script>
<div id="footer">
<div id="footer-text">
Page generated 2022-11-22 10:10:06 HKT, by <a href="https://github.com/wsshin/jemdoc_mathjax" target="blank">jemdoc+MathJax</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
